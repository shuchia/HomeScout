# HomeScout Backend API

FastAPI backend for the HomeScout apartment finder application.

## Features

- ğŸ  Search apartments by city, budget, bedrooms, bathrooms, and property type
- ğŸ¤– AI-powered match scoring using Claude Messages API
- ğŸ“Š Returns top 10 ranked apartment recommendations
- ğŸ”„ CORS enabled for frontend integration
- ğŸ“ Auto-generated API documentation
- ğŸ—„ï¸ PostgreSQL database with JSON fallback mode
- ğŸ•·ï¸ Data collection from Zillow, Apartments.com, and Craigslist
- â° Celery task scheduling for automated scraping
- ğŸ“ˆ Prometheus metrics and monitoring

## Tech Stack

- **FastAPI** - Modern Python web framework
- **Anthropic Claude API** - AI-powered apartment matching
- **PostgreSQL + SQLAlchemy** - Database with async support
- **Celery + Redis** - Task queue for background jobs
- **Apify / ScrapingBee** - Data collection services
- **Pydantic** - Data validation
- **Uvicorn** - ASGI server

## Setup

### 1. Install Dependencies

```bash
# Make sure you're in the backend directory
cd backend

# Install Python dependencies
pip install -r requirements.txt
```

### 2. Configure Environment Variables

Create a `.env` file in the `backend/` directory:

```bash
cp .env.example .env
```

Edit `.env` and add your Anthropic API key:

```
ANTHROPIC_API_KEY=your_actual_api_key_here
FRONTEND_URL=http://localhost:3000
```

**Get your API key:**
1. Go to https://console.anthropic.com/
2. Sign up or log in
3. Navigate to API Keys
4. Create a new key

### 3. Run the Server

```bash
# From the backend directory
uvicorn app.main:app --reload --port 8000
```

The API will be available at `http://localhost:8000`

## Database Setup (Optional)

By default, HomeScout uses a static JSON file for apartment data. To enable PostgreSQL:

### 1. Install PostgreSQL

```bash
# macOS
brew install postgresql

# Ubuntu/Debian
sudo apt-get install postgresql postgresql-contrib
```

### 2. Create Database

```bash
createdb homescout
```

### 3. Configure Environment

Add to your `.env`:

```
USE_DATABASE=true
DATABASE_URL=postgresql+asyncpg://postgres:postgres@localhost:5432/homescout
```

### 4. Run Migrations

```bash
cd backend
alembic upgrade head
```

## Data Collection Setup (Optional)

To enable automated apartment data collection:

### 1. Start Redis

```bash
# macOS
brew install redis
brew services start redis

# Ubuntu/Debian
sudo apt-get install redis-server
sudo systemctl start redis
```

### 2. Configure API Keys

Add to your `.env`:

```
# Apify - for Zillow and Apartments.com
APIFY_API_TOKEN=your_apify_token

# ScrapingBee - for Craigslist
SCRAPINGBEE_API_KEY=your_scrapingbee_key
```

### 3. Start Celery Worker

```bash
# In a separate terminal
cd backend
celery -A app.celery_app worker --loglevel=info
```

### 4. Start Celery Beat (Scheduler)

```bash
# In another terminal
cd backend
celery -A app.celery_app beat --loglevel=info
```

## API Endpoints

### Health Check

```bash
GET /health
```

### Search Apartments

```bash
POST /api/search
Content-Type: application/json

{
  "city": "San Francisco, CA",
  "budget": 3500,
  "bedrooms": 2,
  "bathrooms": 2,
  "property_type": "Apartment, Condo",
  "move_in_date": "2025-12-01",
  "other_preferences": "Pet-friendly, parking, in-unit laundry"
}
```

### Get Apartment Count

```bash
GET /api/apartments/count
```

### Get Apartment Statistics

```bash
GET /api/apartments/stats
```

### Data Collection Admin API

```bash
# Trigger manual scrape job
POST /api/admin/data-collection/jobs
{
  "source": "zillow",
  "city": "San Francisco",
  "state": "CA",
  "max_listings": 100
}

# List scrape jobs
GET /api/admin/data-collection/jobs

# Get job status
GET /api/admin/data-collection/jobs/{job_id}

# List data sources
GET /api/admin/data-collection/sources

# Update source configuration
PUT /api/admin/data-collection/sources/{source_id}

# Get collection metrics
GET /api/admin/data-collection/metrics

# Health check for all services
GET /api/admin/data-collection/health
```

### Prometheus Metrics

```bash
GET /metrics
```

## Testing with curl

```bash
# Health check
curl http://localhost:8000/health

# Search apartments
curl -X POST http://localhost:8000/api/search \
  -H "Content-Type: application/json" \
  -d '{
    "city": "San Francisco, CA",
    "budget": 3500,
    "bedrooms": 2,
    "bathrooms": 2,
    "property_type": "Apartment",
    "move_in_date": "2025-12-01",
    "other_preferences": "Pet-friendly with parking"
  }'

# Trigger a scrape job (requires database mode)
curl -X POST http://localhost:8000/api/admin/data-collection/jobs \
  -H "Content-Type: application/json" \
  -d '{"source": "zillow", "city": "San Francisco", "state": "CA"}'
```

## Interactive API Documentation

Once the server is running, visit:

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

These provide interactive API documentation where you can test endpoints directly in your browser.

## Project Structure

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                    # FastAPI app and endpoints
â”‚   â”œâ”€â”€ database.py                # SQLAlchemy async configuration
â”‚   â”œâ”€â”€ celery_app.py              # Celery configuration
â”‚   â”œâ”€â”€ models.py                  # Pydantic models (API)
â”‚   â”œâ”€â”€ models/                    # SQLAlchemy ORM models
â”‚   â”‚   â”œâ”€â”€ apartment.py
â”‚   â”‚   â”œâ”€â”€ scrape_job.py
â”‚   â”‚   â””â”€â”€ data_source.py
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â””â”€â”€ data_collection.py     # Admin API endpoints
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ claude_service.py      # Claude API integration
â”‚   â”‚   â”œâ”€â”€ apartment_service.py   # Apartment search logic
â”‚   â”‚   â”œâ”€â”€ scrapers/              # Data collection services
â”‚   â”‚   â”‚   â”œâ”€â”€ base_scraper.py
â”‚   â”‚   â”‚   â”œâ”€â”€ apify_service.py
â”‚   â”‚   â”‚   â””â”€â”€ scrapingbee_service.py
â”‚   â”‚   â”œâ”€â”€ normalization/         # Data normalization
â”‚   â”‚   â”‚   â”œâ”€â”€ normalizer.py
â”‚   â”‚   â”‚   â””â”€â”€ address_standardizer.py
â”‚   â”‚   â”œâ”€â”€ deduplication/         # Duplicate detection
â”‚   â”‚   â”‚   â””â”€â”€ deduplicator.py
â”‚   â”‚   â”œâ”€â”€ storage/               # S3 image caching
â”‚   â”‚   â”‚   â””â”€â”€ s3_service.py
â”‚   â”‚   â””â”€â”€ monitoring/            # Metrics and alerts
â”‚   â”‚       â”œâ”€â”€ metrics.py
â”‚   â”‚       â””â”€â”€ alerts.py
â”‚   â”œâ”€â”€ tasks/                     # Celery tasks
â”‚   â”‚   â”œâ”€â”€ scrape_tasks.py
â”‚   â”‚   â””â”€â”€ maintenance_tasks.py
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ apartments.json        # Mock apartment data (fallback)
â”œâ”€â”€ alembic/                       # Database migrations
â”‚   â”œâ”€â”€ env.py
â”‚   â””â”€â”€ versions/
â”œâ”€â”€ .env                           # Environment variables (gitignored)
â”œâ”€â”€ .env.example                   # Example env file
â”œâ”€â”€ alembic.ini                    # Alembic configuration
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## How It Works

### Search Flow
1. **User sends search request** â†’ API receives JSON with search criteria
2. **Filter apartments** â†’ Basic filtering by city, budget, beds, baths, property type
3. **Claude AI scoring** â†’ Filtered apartments are sent to Claude for intelligent matching
4. **Rank & return** â†’ Top 10 apartments sorted by match score are returned

### Data Collection Flow
1. **Scheduled task triggers** â†’ Celery beat schedules scraping jobs
2. **Scraper fetches data** â†’ Apify/ScrapingBee retrieves listings
3. **Normalize data** â†’ Address standardization, field validation
4. **Deduplicate** â†’ Content hashing and fuzzy matching
5. **Store in database** â†’ PostgreSQL with quality scoring

## Development Tips

### Watch for file changes

The `--reload` flag automatically restarts the server when code changes.

### View logs

The server prints logs to the console. Watch for:
- Incoming requests
- Claude API calls
- Any errors

### Running Tests

```bash
cd backend
pytest
```

### Database Migrations

```bash
# Create a new migration
alembic revision --autogenerate -m "Description"

# Apply migrations
alembic upgrade head

# Rollback
alembic downgrade -1
```

### Common Issues

**Issue:** "ANTHROPIC_API_KEY environment variable not set"
**Solution:** Make sure your `.env` file exists and contains your API key.

**Issue:** "Address already in use"
**Solution:** Another process is using port 8000. Kill it or use a different port:
```bash
uvicorn app.main:app --reload --port 8001
```

**Issue:** "No apartments found"
**Solution:** Check that your city name matches the format in `apartments.json` (e.g., "San Francisco, CA" not just "San Francisco")

**Issue:** "Database connection failed"
**Solution:** Ensure PostgreSQL is running and DATABASE_URL is correct in .env

## Support

For issues or questions:
1. Check the API docs at `/docs`
2. Review the code comments
3. Check that your `.env` is configured correctly
